{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação das bibliotecas necessárias e adição de um novo diretório ao caminho de busca dos módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')\n",
    "from utils.SparkRefine import SparkRefine\n",
    "from utils.utils import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria uma instância da classe SparkRefine e outra da classe Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_refine = SparkRefine()\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define uma variável e cria uma visualização temporária para consultas SQL aplicando algumas transformações usando a função 'create_spark_temp_view_with_transform' cujos parâmetros especificam o nome da vizualização, o caminho do arquivo Parquet e as transformações realizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'refined'\n",
    "business_area = 'sei_andamentos'\n",
    "source_system = 'operacional'\n",
    "\n",
    "table_dim_sei_andamentos = \"dim_sei_andamentos\"\n",
    "\n",
    "spark_refine.create_spark_temp_view(\n",
    "    bucket_name=bucket_name,\n",
    "    file_path=f'{business_area}/{source_system}/{table_dim_sei_andamentos}.parquet', \n",
    "    view_name=table_dim_sei_andamentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define uma variável e cria uma visualização temporária para consultas SQL usando a função 'create_spark_temp_view' cujos parâmetros especificam o nome da vizualização e o caminho do arquivo Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'refined'\n",
    "business_area = 'sei_unidades'\n",
    "source_system = 'operacional'\n",
    "\n",
    "table_dim_sei_unidades = \"dim_sei_unidades\"\n",
    "\n",
    "spark_refine.create_spark_temp_view(\n",
    "    bucket_name=bucket_name,\n",
    "    file_path=f'{business_area}/{source_system}/{table_dim_sei_unidades}.parquet', \n",
    "    view_name=table_dim_sei_unidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define uma variável e cria uma visualização temporária para consultas SQL usando a função 'create_spark_temp_view' cujos parâmetros especificam o nome da vizualização e o caminho do arquivo Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'refined'\n",
    "business_area = 'sei_procedimentos'\n",
    "source_system = 'operacional'\n",
    "\n",
    "table_dim_sei_procedimentos = \"dim_sei_procedimentos\"\n",
    "\n",
    "spark_refine.create_spark_temp_view(\n",
    "    bucket_name=bucket_name,\n",
    "    file_path=f'{business_area}/{source_system}/{table_dim_sei_procedimentos}.parquet', \n",
    "    view_name=table_dim_sei_procedimentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define uma variável e cria uma visualização temporária para consultas SQL usando a função 'create_spark_temp_view' cujos parâmetros especificam o nome da vizualização e o caminho do arquivo Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'refined'\n",
    "business_area = 'sei_tipo_procedimentos'\n",
    "source_system = 'operacional'\n",
    "\n",
    "table_dim_sei_tipo_procedimentos = \"dim_sei_tipo_procedimentos\"\n",
    "\n",
    "spark_refine.create_spark_temp_view(\n",
    "    bucket_name=bucket_name,\n",
    "    file_path=f'{business_area}/{source_system}/{table_dim_sei_tipo_procedimentos}.parquet', \n",
    "    view_name=table_dim_sei_tipo_procedimentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliza a função 'generate_parquet_dataframe_from_sql' para transformar o resultado da consulta SQL em um DataFrame PySpark e, em seguida, salvar esse DataFrame em formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_refine.generate_parquet_dataframe_from_sql(\n",
    "    sql=f\"\"\"\n",
    "        SELECT \n",
    "            a.IdProcedimentoFormatado,\n",
    "            u.Sigla,\n",
    "            tp.Nome AS tipo,\n",
    "            DATEDIFF(CURRENT_TIMESTAMP(), TO_DATE(max_dates.max_data, 'dd/MM/yyyy HH:mm:ss')) AS Dias_Sem_Movimentacao\n",
    "        FROM {table_dim_sei_andamentos} a\n",
    "            LEFT JOIN {table_dim_sei_unidades} u ON u.IdUnidade = a.Unidade\n",
    "            LEFT JOIN {table_dim_sei_procedimentos} p ON p.ProcedimentoFormatado = a.IdProcedimentoFormatado\n",
    "            LEFT JOIN {table_dim_sei_tipo_procedimentos} tp ON tp.IdTipoProcedimento = p.TipoProcedimento\n",
    "        INNER JOIN (\n",
    "            SELECT \n",
    "                Unidade, \n",
    "                IdProcedimentoFormatado,\n",
    "                MAX(TO_TIMESTAMP(DataHora, 'dd/MM/yyyy HH:mm:ss')) AS max_data\n",
    "            FROM {table_dim_sei_andamentos}\n",
    "            WHERE \n",
    "                IdProcedimentoFormatado IN (\n",
    "                    SELECT ProcedimentoFormatado FROM {table_dim_sei_procedimentos}\n",
    "                )\n",
    "                AND IdTarefa IN ('2')\n",
    "            GROUP BY \n",
    "                Unidade, IdProcedimentoFormatado\n",
    "        ) max_dates \n",
    "        ON a.Unidade = max_dates.Unidade \n",
    "        AND a.IdProcedimentoFormatado = max_dates.IdProcedimentoFormatado\n",
    "        AND TO_TIMESTAMP(a.DataHora, 'dd/MM/yyyy HH:mm:ss') = max_dates.max_data\n",
    "        WHERE \n",
    "            a.IdProcedimentoFormatado IN (\n",
    "                SELECT ProcedimentoFormatado FROM {table_dim_sei_procedimentos}\n",
    "            )\n",
    "            AND a.IdTarefa IN ('2')\n",
    "            AND u.Sigla != 'AL-SAI'\n",
    "            AND tp.Nome IN (\n",
    "                'Adesão à Ata de Registro de Preços', 'Compra de Material e Contratação de Serviços',\n",
    "                'Compra Direta', 'Contratação por Pregão Eletrônico',\n",
    "                'Contratação por Pregão Presencial', 'Contratação/Pagamento de Serviços',\n",
    "                'Contrato', 'Controle de Pagamento de Registro de Preço',\n",
    "                'Convênios', 'Dispensa de Licitação',\n",
    "                'Dispensa de Licitação - Extrato', 'Inexigibilidade de Licitação',\n",
    "                'Licitação', 'Pagamento de Despesas Continuadas/Contratos',\n",
    "                'Prestação de Serviços', 'Projeto/Proposta',\n",
    "                'Projetos/Obras e Construções', 'Registro de Preço'\n",
    "            )\n",
    "        ORDER BY max_dates.max_data DESC\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a variável com o novo nome da tabela e utiliza a função 'send_dataframe_to_minio' para enviar a tabela para armazenamento no MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'refined'\n",
    "business_area = 'sei_processos_parados'\n",
    "source_system = 'operacional'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_fato_sei_processos_parados = \"fato_sei_processos_parados\"\n",
    "\n",
    "spark_refine.send_dataframe_to_minio(\n",
    "    f'{business_area}/{source_system}/{table_fato_sei_processos_parados}.parquet', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
