{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0c4091-c4a0-4aa6-8775-83382a027d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')\n",
    "\n",
    "from utils.SparkStandardize import SparkStandardize\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783aa11e-cc25-4ea4-8673-2d852c7a32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos encontrados:\n",
      "['sei_andamentos/operacional/andamentos/20240328/andamentos_20240328.json', 'sei_tipo_procedimentos/operacional/tipoprocedimentos/20240328/tipoprocedimentos_20240328.json', 'sei_unidades/operacional/unidades/20240328/unidades_20240328.json', 'sei_usuarios/operacional/usuarios/20240328/usuarios_20240328.json']\n",
      "\n",
      "----- Processando arquivo unidades_20240328.json -----\n",
      "Path: sei_unidades/operacional/unidades/20240328/unidades_20240328.json\n",
      "Fazendo download da tabela da camada RAW...\n",
      "Carregando tabela para o Spark...\n",
      "--- VERIFICAÇÃO DE INTEGRIDADE ---\n",
      "OK!\n",
      "--- ---\n",
      "Adicionando metadados\n",
      "Enviando tabela normalizada para o MinIO\n",
      "Convertendo o Dataframe do Spark para Parquet e salvando localmente\n",
      "Salvo com sucesso\n",
      "Removendo arquivos pré-existentes da STANDARDIZED em:\tsei_unidades/operacional/unidades/20240328/unidades_20240328.parquet\n",
      "Fazendo o upload dos arquivos Parquet locais para a camada destino no MinIO\n",
      "Bucket existente... enviando arquivos\n",
      "Bucket existente... enviando arquivos\n",
      "Bucket existente... enviando arquivos\n",
      "Bucket existente... enviando arquivos\n",
      "Enviado com sucesso\n",
      "----- Processamento Finalizado! -----\n"
     ]
    }
   ],
   "source": [
    "sparkStandardize = SparkStandardize(check_table_integrity=True)\n",
    "duplicated_tables = sparkStandardize.normalize_table_files_to_parquet('json', business_area='sei_unidades')\n",
    "del sparkStandardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98aadfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a7d3d-65e2-4205-904c-bbb5651703c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
